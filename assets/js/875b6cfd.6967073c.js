"use strict";(self.webpackChunkradxa_web=self.webpackChunkradxa_web||[]).push([[3262],{3905:(e,t,r)=>{r.d(t,{Zo:()=>u,kt:()=>f});var o=r(7294);function i(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function n(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,o)}return r}function a(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?n(Object(r),!0).forEach((function(t){i(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):n(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,o,i=function(e,t){if(null==e)return{};var r,o,i={},n=Object.keys(e);for(o=0;o<n.length;o++)r=n[o],t.indexOf(r)>=0||(i[r]=e[r]);return i}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(o=0;o<n.length;o++)r=n[o],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(i[r]=e[r])}return i}var p=o.createContext({}),d=function(e){var t=o.useContext(p),r=t;return e&&(r="function"==typeof e?e(t):a(a({},t),e)),r},u=function(e){var t=d(e.components);return o.createElement(p.Provider,{value:t},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},l=o.forwardRef((function(e,t){var r=e.components,i=e.mdxType,n=e.originalType,p=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),c=d(r),l=i,f=c["".concat(p,".").concat(l)]||c[l]||m[l]||n;return r?o.createElement(f,a(a({ref:t},u),{},{components:r})):o.createElement(f,a({ref:t},u))}));function f(e,t){var r=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var n=r.length,a=new Array(n);a[0]=l;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s[c]="string"==typeof e?e:i,a[1]=s;for(var d=2;d<n;d++)a[d]=r[d];return o.createElement.apply(null,a)}return o.createElement.apply(null,r)}l.displayName="MDXCreateElement"},8828:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>p,contentTitle:()=>a,default:()=>m,frontMatter:()=>n,metadata:()=>s,toc:()=>d});var o=r(7462),i=(r(7294),r(3905));const n={sidebar_label:"Radxa Fogwise 1684X Mini",sidebar_position:1,sidebar_custom_props:{string_url:"products/fogwise/bm1684m",product_name:"Fogwise BM168M",product_full_name:"Radxa Fogwise BM168M",product_code:"fw190",product_line:"product",product_feature_zh:"\u4eba\u5de5\u667a\u80fd\u5fae\u578b\u670d\u52a1\u5668",product_feature_en:"Edge AI Micro Server",sub_feature_zh:["\u652f\u6301\u751f\u6210\u5f0fAI\u90e8\u7f72","32TOPS@INT8 \u7b97\u529b"],sub_feature_en:["Support for generative AI deployment","32TOPS@INT8"],product_introduction_zh:"Radxa Fogwise BM168M \u662f\u4e00\u6b3e\u5177\u5907\u9ad8\u8fbe32TOPS@INT8\u7b97\u529b\u7684\u5d4c\u5165\u5f0f\u4eba\u5de5\u667a\u80fd\u5fae\u578b\u670d\u52a1\u5668\uff0c\u652f\u6301\u591a\u79cd\u7cbe\u5ea6\uff08INT8, FP16/BF16, FP32)\uff0c\u652f\u6301\u672c\u5730\u5927\u6a21\u578b\u3001\u6587\u751f\u56fe\u7b49\u591a\u79cd\u4e3b\u6d41AI\u6a21\u578b\u90e8\u7f72\uff0c\u914d\u5907\u7684\u94dd\u5408\u91d1\u5916\u58f3\u4f7f\u5176\u80fd\u90e8\u7f72\u5728\u4e25\u82db\u73af\u5883\u3002",product_introduction_en:"Radxa Fogwise BM168M is an embedded AI micro-server with up to 32TOPS@INT8 arithmetic power, supports multiple precision (INT8, FP16/BF16, FP32), supports multiple mainstream AI model deployments such as private GPT, Text-to-image, etc., and is equipped with aluminum alloy casing that enables it to be deployed in harsh environments.",thumbnail_picture:"/fogwise/bm168m/thumb_radxa_fogwise_bm168m.webp",banner_picture:"/fogwise/bm168m/banner_radxa_fogwise_bm168m.webp",spec_pictures:["/fogwise/bm168m/spec_radxa_fogwise_bm168m_01.webp","/fogwise/bm168m/spec_radxa_fogwise_bm168m_02.webp"],marked_picture:["/fogwise/bm168m/mark_fogwise_bm168m.webp"],parameter:{SoC:["SOPHON BM1684X"],CPU:["Octa-core Arm\xae Cortex\xae-A53 (ARMv8)"],TPU:["Tensor Processing Unit with Computational Power: up to 32TOPS (INT8), 16TFLOPS (FP16/BF16) and 2TFLOPS (FP32)","Support for leading Deep learning frameworks including TensorFlow, Caffe, PyTorch, Paddle, ONNX, MXNet, Tengine, and DarkNet"],RAM:["16GB LPDDR4X"],Storage:["64GB Onboard eMMC","M.2 M Key Connector supports 2230 NVMe SSD","Micro SD Card"],Multimedia:["Supports Decoding of 32 Channels of H.265/H.264 1080p@25fps Video","Fully Process 32 Channels of High-Definition 1080P@25fps Videos, Involving Decoding and AI Analysis","Supports Encoding of 12 Channels of H.265/H.264 1080p@25fps Videos","JPEG: 1080P@600fps, Supporting a Maximum of 32768 x 32768","Supports video post-processing, including image CSC, Resize, Crop, Padding, Border, Font, Contrast, and Brightness adjustments."],Wireless:["M.2 E Key for Wireless Module(WiFi & BT)"],Ethernet:["2x Gigabit Ethernet without PoE support"],USB:["2x USB 3.0 HOST","1x USB Type-C Debug UART"],Power:["1x USB Type-C 20V Power"],Dimension:["104mm x 84mm x 52mm"],"Operation Temperature":["J1 model(BM1684x): -40\xb0C to +105\xb0C"],"Compliance Certification":["FCC / CE"]},product_docs:[{title:"Coming Soon",info:"Getting Started with your Radxa Fogwise BM168M",url:""}],product_downloads:{},related_products:[],distributors:[],metadata:{title:"Radxa Fogwise BM1684M",description:"Radxa Fogwise BM168M is an embedded AI micro-server with up to 32TOPS@INT8 arithmetic power, supports multiple precision (INT8, FP16/BF16, FP32), supports multiple mainstream AI model deployments such as private GPT, Text-to-image, etc., and is equipped with aluminum alloy casing that enables it to be deployed in harsh environments.",keywords:"Edge AI Micro Server, BM1684X, Radxa Fogwise BM168M"}}},a=void 0,s={unversionedId:"products/fogwise/fw190",id:"products/fogwise/fw190",title:"fw190",description:"",source:"@site/docs/products/fogwise/fw190.md",sourceDirName:"products/fogwise",slug:"/products/fogwise/fw190",permalink:"/docs/products/fogwise/fw190",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_label:"Radxa Fogwise 1684X Mini",sidebar_position:1,sidebar_custom_props:{string_url:"products/fogwise/bm1684m",product_name:"Fogwise BM168M",product_full_name:"Radxa Fogwise BM168M",product_code:"fw190",product_line:"product",product_feature_zh:"\u4eba\u5de5\u667a\u80fd\u5fae\u578b\u670d\u52a1\u5668",product_feature_en:"Edge AI Micro Server",sub_feature_zh:["\u652f\u6301\u751f\u6210\u5f0fAI\u90e8\u7f72","32TOPS@INT8 \u7b97\u529b"],sub_feature_en:["Support for generative AI deployment","32TOPS@INT8"],product_introduction_zh:"Radxa Fogwise BM168M \u662f\u4e00\u6b3e\u5177\u5907\u9ad8\u8fbe32TOPS@INT8\u7b97\u529b\u7684\u5d4c\u5165\u5f0f\u4eba\u5de5\u667a\u80fd\u5fae\u578b\u670d\u52a1\u5668\uff0c\u652f\u6301\u591a\u79cd\u7cbe\u5ea6\uff08INT8, FP16/BF16, FP32)\uff0c\u652f\u6301\u672c\u5730\u5927\u6a21\u578b\u3001\u6587\u751f\u56fe\u7b49\u591a\u79cd\u4e3b\u6d41AI\u6a21\u578b\u90e8\u7f72\uff0c\u914d\u5907\u7684\u94dd\u5408\u91d1\u5916\u58f3\u4f7f\u5176\u80fd\u90e8\u7f72\u5728\u4e25\u82db\u73af\u5883\u3002",product_introduction_en:"Radxa Fogwise BM168M is an embedded AI micro-server with up to 32TOPS@INT8 arithmetic power, supports multiple precision (INT8, FP16/BF16, FP32), supports multiple mainstream AI model deployments such as private GPT, Text-to-image, etc., and is equipped with aluminum alloy casing that enables it to be deployed in harsh environments.",thumbnail_picture:"/fogwise/bm168m/thumb_radxa_fogwise_bm168m.webp",banner_picture:"/fogwise/bm168m/banner_radxa_fogwise_bm168m.webp",spec_pictures:["/fogwise/bm168m/spec_radxa_fogwise_bm168m_01.webp","/fogwise/bm168m/spec_radxa_fogwise_bm168m_02.webp"],marked_picture:["/fogwise/bm168m/mark_fogwise_bm168m.webp"],parameter:{SoC:["SOPHON BM1684X"],CPU:["Octa-core Arm\xae Cortex\xae-A53 (ARMv8)"],TPU:["Tensor Processing Unit with Computational Power: up to 32TOPS (INT8), 16TFLOPS (FP16/BF16) and 2TFLOPS (FP32)","Support for leading Deep learning frameworks including TensorFlow, Caffe, PyTorch, Paddle, ONNX, MXNet, Tengine, and DarkNet"],RAM:["16GB LPDDR4X"],Storage:["64GB Onboard eMMC","M.2 M Key Connector supports 2230 NVMe SSD","Micro SD Card"],Multimedia:["Supports Decoding of 32 Channels of H.265/H.264 1080p@25fps Video","Fully Process 32 Channels of High-Definition 1080P@25fps Videos, Involving Decoding and AI Analysis","Supports Encoding of 12 Channels of H.265/H.264 1080p@25fps Videos","JPEG: 1080P@600fps, Supporting a Maximum of 32768 x 32768","Supports video post-processing, including image CSC, Resize, Crop, Padding, Border, Font, Contrast, and Brightness adjustments."],Wireless:["M.2 E Key for Wireless Module(WiFi & BT)"],Ethernet:["2x Gigabit Ethernet without PoE support"],USB:["2x USB 3.0 HOST","1x USB Type-C Debug UART"],Power:["1x USB Type-C 20V Power"],Dimension:["104mm x 84mm x 52mm"],"Operation Temperature":["J1 model(BM1684x): -40\xb0C to +105\xb0C"],"Compliance Certification":["FCC / CE"]},product_docs:[{title:"Coming Soon",info:"Getting Started with your Radxa Fogwise BM168M",url:""}],product_downloads:{},related_products:[],distributors:[],metadata:{title:"Radxa Fogwise BM1684M",description:"Radxa Fogwise BM168M is an embedded AI micro-server with up to 32TOPS@INT8 arithmetic power, supports multiple precision (INT8, FP16/BF16, FP32), supports multiple mainstream AI model deployments such as private GPT, Text-to-image, etc., and is equipped with aluminum alloy casing that enables it to be deployed in harsh environments.",keywords:"Edge AI Micro Server, BM1684X, Radxa Fogwise BM168M"}}}},p={},d=[],u={toc:d},c="wrapper";function m(e){let{components:t,...r}=e;return(0,i.kt)(c,(0,o.Z)({},u,r,{components:t,mdxType:"MDXLayout"}))}m.isMDXComponent=!0}}]);